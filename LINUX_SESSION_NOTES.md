# Linux Session Notes - Para PrÃ³ximo Boot Ubuntu

> **IMPORTANTE**: Este arquivo foi criado na sessÃ£o Windows (2026-02-06).
> Leia isto quando instalar Claude Code no Ubuntu amanhÃ£.

---

## ğŸ–¥ï¸ Context: Dual-Boot Setup

Esta Ã© a **mesma mÃ¡quina fÃ­sica**:
- **CPU**: AMD Ryzen 9 7900X (12c/24t)
- **GPU**: AMD Radeon RX 6750 XT 12GB
- **OS atual**: Windows â†’ PrÃ³xima sessÃ£o: Ubuntu (Linux)
- **Projeto**: Mesmo cÃ³digo em ambos os OS

---

## ğŸ“Š Performance Esperada no Ubuntu

### Image Generation (GPU com ROCm)
| ResoluÃ§Ã£o | Steps | Tempo Esperado |
|-----------|-------|----------------|
| 512Ã—512   | 20    | **6-7 segundos** âš¡ |
| 768Ã—768   | 30    | **~30 segundos** |

**ComparaÃ§Ã£o Windows vs Linux (mesmo hardware):**
- Windows + CPU/DirectML: ~30s para 512Ã—512
- **Linux + ROCm: 6-7s para 512Ã—512** (5x mais rÃ¡pido!)

### Modelos que Funcionam
- âœ… `stable-diffusion-1.5`
- âœ… `dreamshaper-8`

---

## ğŸ—ï¸ Arquitetura no Ubuntu (A Validar)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AMD RX 6750 XT (GPU via ROCm)     â”‚
â”‚   â””â”€ Image Generation               â”‚  Port 5001
â”‚      (ultra_optimized_server.py)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ryzen 9 7900X (CPU)               â”‚
â”‚   â””â”€ LLM Chat Inference             â”‚  Port: ???
â”‚      (modelo local)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Node.js Backend                   â”‚  Port 3000
â”‚   Frontend React                    â”‚  Port 5173
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SeparaÃ§Ã£o Inteligente:**
- GPU dedicada para imagens (nÃ£o atrapalha CPU)
- CPU para LLM (nÃ£o compete com GPU)
- Ambos podem rodar simultaneamente

---

## ğŸ¯ Tarefas para SessÃ£o Ubuntu

### 1. Validar Image Generation
```bash
cd ~/Projetos/APIBR2  # ajustar path conforme necessÃ¡rio

# Iniciar servidor Python
cd integrations
python3 ultra_optimized_server.py

# Verificar logs:
# âœ… Deve mostrar: "AMD GPU detected - running with ROCm"
# âŒ NÃƒO deve mostrar: "DirectML" ou "Using CPU"
```

**Testar geraÃ§Ã£o:**
```bash
curl -X POST http://localhost:3000/api/v1/image/generate \
  -H "Content-Type: application/json" \
  -H "x-api-key: dev-key-1" \
  -d '{
    "prompt": "beautiful mountain landscape",
    "model": "stable-diffusion-1.5",
    "num_inference_steps": 20,
    "width": 512,
    "height": 512
  }'

# Tempo esperado: 6-7 segundos (nÃ£o 30s!)
```

### 2. Descobrir e Documentar LLM Chat

**Perguntas a responder:**
- [ ] Qual modelo LLM estÃ¡ rodando? (Llama? Mistral? Outro?)
- [ ] Em qual porta? (5003? 8000? Outra?)
- [ ] Qual arquivo Python roda o LLM?
- [ ] Como acessar via API REST?
- [ ] Como acessar via frontend?
- [ ] Qual tempo de resposta tÃ­pico?

**Procurar por:**
```bash
# Procurar servidores LLM
find . -name "*llm*.py" -o -name "*chat*.py" -o -name "*ollama*.py"

# Verificar portas em uso
netstat -tuln | grep LISTEN

# Procurar no cÃ³digo
grep -r "llm\|chat\|ollama" --include="*.py" integrations/
```

### 3. Validar Uso SimultÃ¢neo
- [ ] Rodar geraÃ§Ã£o de imagem E chat LLM ao mesmo tempo
- [ ] Confirmar que nÃ£o hÃ¡ conflito de recursos
- [ ] Medir performance de cada um

### 4. Documentar API Remota
- [ ] Endpoints do LLM chat
- [ ] Exemplos de uso via curl
- [ ] IntegraÃ§Ã£o com frontend
- [ ] Adicionar ao `CROSS_PLATFORM.md`

---

## ğŸ“ Arquivos Criados Hoje (Windows)

Estes arquivos JÃ estÃ£o no projeto e estarÃ£o disponÃ­veis no Ubuntu:

1. `.gitattributes` - NormalizaÃ§Ã£o de line endings
2. `CROSS_PLATFORM.md` - Guia completo Windows/Linux
3. `COMPATIBILITY_IMPROVEMENTS.md` - Changelog de melhorias
4. `README.md` - Atualizado com info cross-platform
5. `CLAUDE.md` - Atualizado com aviso de compatibilidade
6. **Este arquivo** (`LINUX_SESSION_NOTES.md`)

---

## ğŸš€ Quick Start no Ubuntu

```bash
# 1. Navegar ao projeto
cd ~/Projetos/APIBR2  # ajustar conforme seu path

# 2. Verificar git
git status
git pull  # se necessÃ¡rio

# 3. Dar permissÃ£o aos scripts
chmod +x start_all.sh stop_apibr2.sh

# 4. Iniciar tudo
./start_all.sh

# OU iniciar manualmente:
# Terminal 1: Backend
cd backend && npm run dev

# Terminal 2: Python Image Server
cd integrations && python3 ultra_optimized_server.py

# Terminal 3: Frontend
cd frontend && npm run dev

# Terminal 4: ??? (descobrir servidor LLM)
```

---

## ğŸ” O Que Procurar

### Frontend React
- Verificar se hÃ¡ aba/seÃ§Ã£o de "Chat" ou "LLM"
- URL: http://localhost:5173

### Backend Routes
```bash
# Procurar rotas relacionadas a chat/LLM
grep -r "chat\|llm" backend/src/routes/
grep -r "chat\|llm" backend/src/controllers/
```

### Integrations
```bash
# Listar servidores Python
ls -la integrations/*server*.py
ls -la integrations/*llm*.py
ls -la integrations/*chat*.py
```

---

## ğŸ“ DocumentaÃ§Ã£o a Criar

ApÃ³s descobrir o sistema LLM, documentar:

1. **API Endpoints** (adicionar ao README.md)
   ```markdown
   | Chat LLM | POST /api/chat | Send message to local LLM |
   ```

2. **Exemplos de uso** (adicionar ao README.md)
   ```bash
   curl -X POST http://localhost:XXXX/chat \
     -H "Content-Type: application/json" \
     -d '{"message": "Hello, how are you?"}'
   ```

3. **Performance** (adicionar ao CROSS_PLATFORM.md)
   ```markdown
   | LLM Chat (CPU) | Response time: X seconds |
   ```

---

## ğŸ¯ Objetivo da SessÃ£o Ubuntu

1. âœ… Validar ROCm estÃ¡ funcionando (6-7s para imagens)
2. âœ… Descobrir e documentar sistema LLM chat
3. âœ… Confirmar arquitetura GPU+CPU separada
4. âœ… Documentar API remota completa
5. âœ… Atualizar guias de compatibilidade

---

## ğŸ’¡ Lembrete

- Esta mÃ¡quina JÃ TEM ROCm instalado (vocÃª mencionou que funciona)
- Performance de 6-7s JÃ FOI observada antes
- LLM chat JÃ EXISTE e funciona
- SÃ³ precisamos DOCUMENTAR tudo isso!

---

**Criado em**: 2026-02-06 (sessÃ£o Windows)
**Para ser lido**: PrÃ³xima sessÃ£o Ubuntu com Claude Code recÃ©m-instalado
**Autor**: Claude Code (Windows session)

Boa noite! ğŸ§ğŸ’¤
