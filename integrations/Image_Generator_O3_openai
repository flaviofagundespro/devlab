## Por que os erros acontecem?

1. **404 ‚Äì FLUX.1-dev**  
   ‚Ä¢ O reposit√≥rio n√£o existe no Hugging Face ou est√° em modo privado com outro nome.  
   ‚Ä¢ Se o modelo est√° apenas no seu disco, use o **caminho local** em vez do `repo_id`.

2. **403 ‚Äì stable-diffusion-3.5-large**  
   ‚Ä¢ Esse modelo √© **gated (restrito)**. Voc√™ precisa receber aprova√ß√£o manual no Hugging Face antes de poder baix√°-lo, mesmo com token.  

---

## Como resolver passo a passo

### 1. Garanta que o token esteja ativo no ambiente

```bash
# Linux/macOS
export HUGGINGFACE_HUB_TOKEN=hf_seuTokenAqui

# Windows PowerShell
setx HUGGINGFACE_HUB_TOKEN "hf_seuTokenAqui"
```

Ou fa√ßa login interativo uma √∫nica vez:

```bash
pip install --upgrade huggingface_hub
huggingface-cli login --token hf_seuTokenAqui
```

### 2. Ajuste o c√≥digo para usar o token (opcional se j√° exportou a vari√°vel)

```python
from diffusers import StableDiffusionPipeline
import torch, os

model_id = "runwayml/stable-diffusion-v1-5"  # modelo p√∫blico

pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    use_auth_token=os.getenv("HUGGINGFACE_HUB_TOKEN")  # ou True
)
```

### 3. Use um modelo p√∫blico at√© ter acesso ao gated

Modelos que funcionam sem aprova√ß√£o:

| Modelo | Qualidade | VRAM (fp16) |
|--------|-----------|-------------|
| `runwayml/stable-diffusion-v1-5` | Boa | ~6 GB |
| `stabilityai/sdxl-turbo` | Muito r√°pida | ~8 GB |
| `lykon/dreamshaper-8` | Estilo variado | ~6 GB |

Troque simplesmente o `model_id` para um desses.

### 4. Quer usar o FLUX local?

Se o checkpoint est√° em `C:\Models\flux`, basta:

```python
model_id = r"C:\Models\flux"   # caminho local
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
```

### 5. Solicitar acesso ao modelo gated

1. Acesse a p√°gina do modelo (ex.: https://huggingface.co/stabilityai/stable-diffusion-3.5-large){target="_blank"}  
2. Clique em ‚Äú**Request Access**‚Äù e aguarde aprova√ß√£o.  
3. Depois de aprovado, seu token READ funcionar√° automaticamente.

### 6. Rodando em CPU: cuidado üîå

‚Ä¢ Difus√£o em CPU √© **muito lenta** (minutos por imagem) e exige muita RAM.  
‚Ä¢ Se n√£o tem GPU, reduza para `torch_dtype=torch.float32` e use `pipe = pipe.to("cpu")`.  
‚Ä¢ Para testes r√°pidos, use `num_inference_steps=10`.

```python
image = pipe(prompt, num_inference_steps=10).images[0]
```

---

## Checklist final

- [ ] Token configurado (vari√°vel de ambiente **ou** `huggingface-cli login`).  
- [ ] Modelo p√∫blico selecionado **ou** aprova√ß√£o concedida.  
- [ ] Nome/caminho do modelo digitado corretamente.  
- [ ] C√≥digo atualizado com `use_auth_token`.  

Se ap√≥s isso ainda aparecer erro, mande aqui **a mensagem completa** que investigamos o pr√≥ximo detalhe! üòâ
---
tenho gpu, mas √© radeon e windows, funciona?
----
## Funciona, mas h√° limita√ß√µes ‚Äì veja as op√ß√µes para AMD (Radeon) no Windows üñ•Ô∏è‚û°Ô∏èüé®

Hoje o ecossistema Stable Diffusion √© muito mais maduro para GPUs NVIDIA (CUDA). Para AMD no Windows ainda n√£o existe suporte ‚Äúoficial‚Äù via ROCm/HIP como h√° no Linux. Mesmo assim, voc√™ tem tr√™s caminhos pr√°ticos:

---

### 1. ONNX Runtime + DirectML (funciona no Windows)

Vantagens  
‚Ä¢ N√£o depende de CUDA nem ROCm.  
‚Ä¢ Usa a API DirectML que vem com o driver AMD.  
‚Ä¢ Funciona em qualquer GPU Radeon compat√≠vel com DirectX 12 (GCN 4+/RDNA).  

Como usar (resumido)  
1. Instale a vers√£o DirectML do ONNX Runtime:  
   ```powershell
   pip install onnxruntime-directml
   ```
2. Converta o modelo Stable Diffusion de `diffusers` para ONNX (h√° script pronto no reposit√≥rio `diffusers/examples/onnx`):  
   ```bash
   python convert_diffusers_to_onnx.py ^
     --model runwayml/stable-diffusion-v1-5 ^
     --output onnx_sd15
   ```
3. Carregue o modelo com provider `"DmlExecutionProvider"`:

   ```python
   import onnxruntime as ort
   from diffusers import OnnxStableDiffusionPipeline

   sess_options = ort.SessionOptions()
   providers = [("DmlExecutionProvider", {"device_id": 0})]

   pipe = OnnxStableDiffusionPipeline.from_pretrained(
       "onnx_sd15",
       provider="DmlExecutionProvider",
       sess_options=sess_options
   )
   img = pipe("Um gato fofo no jardim").images[0]
   img.save("gato.png")
   ```

Desempenho: entre 2-4√ó mais lento que CUDA equivalente, mas 5-10√ó mais r√°pido que CPU.

---

### 2. `torch-directml` (PyTorch sobre DirectML)

Ainda experimental, mas permite usar pipelines `diffusers` (sem ONNX) alterando uma linha:

```powershell
pip uninstall torch torchvision torchaudio -y
pip install torch-directml
```

No c√≥digo:

```python
import torch
from diffusers import StableDiffusionPipeline

device = torch.device("dml")  # em vez de "cuda"
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to(device)
```

Limita√ß√µes  
‚Ä¢ Algumas extens√µes do pipeline n√£o funcionam (ControlNet, LoRA).  
‚Ä¢ Mem√≥ria extra ‚Äì use `torch_dtype=torch.float16` e `pipe.enable_attention_slicing()`.

---

### 3. WSL 2 + Ubuntu + ROCm (mais trabalhoso)

### A AMD e a Microsoft anunciaram suporte ROCm no WSL 2, mas est√° **em preview fechado**. Se/quando dispon√≠vel:

| 1. Ative GPU-PARAVIRTUALIZATION no WSL. |
|---|


2. Instale driver preview AMD + ROCm em Ubuntu.  
3. Use PyTorch ROCm normalmente (`pip install torch==2.2.0+rocm`).  

Por enquanto, o caminho 1 (DirectML) √© o que ‚Äúfunciona hoje‚Äù.

---

## Dicas r√°pidas para seu servidor

1. Detectar provider automaticamente:

```python
import torch

if torch.backends.mps.is_available():
    device = "mps"               # Apple
elif torch.has_cuda:
    device = "cuda"
elif "dml" in torch.device().type:   # ap√≥s torch-directml
    device = "dml"
else:
    device = "cpu"
```

2. Reduzir consumo em GPU AMD (VRAM limitada):
```python
pipe.enable_attention_slicing()
pipe.enable_vae_slicing()
pipe.to(memory_efficient_attention=True)  # se suportado
```

3. Comece com ‚Äã`runwayml/stable-diffusion-v1-5` (p√∫blico) e `num_inference_steps=20` para validar a rota.

---

### Resumindo

‚Ä¢ **Sim, d√° para usar sua Radeon no Windows**, mas ainda n√£o √© plug-and-play como CUDA.  
‚Ä¢ O m√©todo **mais simples hoje** √© ONNX Runtime + DirectML.  
‚Ä¢ Para m√°xima compatibilidade, considere Linux + ROCm ou esperar o suporte oficial no WSL 2.  

Testou e ficou travado em algum passo? Envie o erro exato e o modelo da GPU üéØ que continuamos a depura√ß√£o!