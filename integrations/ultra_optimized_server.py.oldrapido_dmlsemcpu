#!/usr/bin/env python3
"""
APIBR2 - Ultra Optimized Image Generation Server v2.0
Servidor com gerenciamento avan√ßado de mem√≥ria e otimiza√ß√µes extremas
Otimizado especialmente para AMD Radeon RX 6750 XT
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from pathlib import Path
import torch
import uuid
import time
import os
import base64
import logging
import gc
from datetime import datetime

# Configurar PyTorch para usar todos os cores da CPU
# Ryzen 9 7900X tem 12 cores / 24 threads
num_threads = os.cpu_count() or 12  # Fallback para 12 se n√£o detectar
torch.set_num_threads(num_threads)
torch.set_num_interop_threads(num_threads)

# Habilitar otimiza√ß√µes de CPU
if hasattr(torch, 'set_float32_matmul_precision'):
    torch.set_float32_matmul_precision('high')

logger = logging.getLogger(__name__)
logger.info(f"üîß PyTorch configurado para usar {num_threads} threads")

# Vari√°veis de ambiente para otimiza√ß√£o de CPU (antes de importar diffusers)
os.environ['OMP_NUM_THREADS'] = str(num_threads)
os.environ['MKL_NUM_THREADS'] = str(num_threads)
os.environ['OPENBLAS_NUM_THREADS'] = str(num_threads)
os.environ['VECLIB_MAXIMUM_THREADS'] = str(num_threads)
os.environ['NUMEXPR_NUM_THREADS'] = str(num_threads)

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="APIBR2 Ultra Optimized Generator v2.1", version="2.1.0")

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configura√ß√µes
OUT_DIR = Path("generated_images")
OUT_DIR.mkdir(exist_ok=True)

# For√ßar CPU se DirectML estiver dando problemas
FORCE_CPU = os.getenv("FORCE_CPU", "false").lower() == "true"

# Preferir CPU sobre DirectML (DirectML √© muito lento em alguns casos)
PREFER_CPU = os.getenv("PREFER_CPU", "false").lower() == "true"

# Cache de pipelines carregados
pipes = {}

class ImageRequest(BaseModel):
    prompt: str
    model: str = "runwayml/stable-diffusion-v1-5"
    steps: int = 10  # Reduzido para 10 (melhor velocidade/qualidade)
    guidance_scale: float = 7.5
    width: int = 512
    height: int = 512
    size: str = "512x512"
    scheduler: str = "auto"  # auto, dpm++, euler_a, ddim
    
    def __init__(self, **data):
        super().__init__(**data)
        # Se size foi fornecido, usar para width e height
        if 'size' in data and data['size']:
            try:
                parts = data['size'].split('x')
                if len(parts) == 2:
                    self.width = int(parts[0])
                    self.height = int(parts[1])
            except:
                pass

def detect_device():
    """Detectar o melhor device dispon√≠vel com prefer√™ncias otimizadas"""
    try:
        # Se FORCE_CPU estiver ativado, usar CPU
        if FORCE_CPU:
            logger.info("‚ö†Ô∏è FORCE_CPU ativado - usando CPU")
            return "cpu"
        
        # Se PREFER_CPU estiver ativado, verificar se vale a pena usar GPU
        if PREFER_CPU:
            logger.info("‚ö†Ô∏è PREFER_CPU ativado - usando CPU (melhor para desenvolvimento)")
            return "cpu"
        
        # Tentar DirectML primeiro (AMD GPU)
        try:
            if hasattr(torch, 'dml') and torch.dml.is_available():
                logger.info("‚úÖ AMD GPU detectada - usando DirectML")
                logger.warning("‚ö†Ô∏è DirectML pode ser lento. Para usar CPU (mais r√°pido para testes): set PREFER_CPU=true")
                return "dml"
            else:
                try:
                    import torch_directml
                    if torch_directml.is_available():
                        logger.info("‚úÖ AMD GPU detectada via torch-directml - usando DirectML")
                        logger.warning("‚ö†Ô∏è DirectML pode ser lento. Para usar CPU (mais r√°pido para testes): set PREFER_CPU=true")
                        return "dml"
                except ImportError:
                    logger.warning("‚ö†Ô∏è torch-directml n√£o instalado")
                    logger.info("üí° Para usar GPU AMD: pip install torch-directml")
        except Exception as e:
            logger.debug(f"DirectML check failed: {e}")
        
        # Tentar CUDA (NVIDIA GPU)
        if torch.cuda.is_available():
            logger.info("‚úÖ NVIDIA GPU detectada - usando CUDA")
            return "cuda"
        
        # Tentar MPS (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            logger.info("‚úÖ Apple Silicon detectado - usando MPS")
            return "mps"
        
        # Fallback para CPU
        logger.info("‚ÑπÔ∏è Usando CPU - Ryzen 9 7900X √© r√°pido para desenvolvimento!")
        logger.info("üí° Tempo estimado: 20-40s para 512x512 com 10 steps")
        return "cpu"
    except Exception as e:
        logger.warning(f"Erro ao detectar device: {e}, usando CPU")
        return "cpu"

def get_scheduler(pipe, scheduler_name, device="cpu"):
    """Configurar scheduler otimizado"""
    from diffusers import (
        DPMSolverMultistepScheduler,
        EulerAncestralDiscreteScheduler,
        DDIMScheduler,
        PNDMScheduler
    )
    
    # Escolher scheduler baseado no device e prefer√™ncia
    if scheduler_name == "auto":
        # DPM++ √© o melhor para qualidade/velocidade
        scheduler_name = "dpm++"
    
    schedulers = {
        "dpm++": DPMSolverMultistepScheduler,
        "euler_a": EulerAncestralDiscreteScheduler,
        "ddim": DDIMScheduler,
        "pndm": PNDMScheduler
    }
    
    scheduler_class = schedulers.get(scheduler_name, DPMSolverMultistepScheduler)
    
    try:
        pipe.scheduler = scheduler_class.from_config(pipe.scheduler.config)
        logger.info(f"Using scheduler: {scheduler_name}")
    except Exception as e:
        logger.warning(f"Could not set scheduler {scheduler_name}: {e}, using default")
    
    return pipe

def get_model_config(model_name, device="cpu"):
    """Configura√ß√µes espec√≠ficas por modelo e device - OTIMIZADO"""
    
    # Configura√ß√µes otimizadas por device
    if device == "dml":
        base_steps = 12  # DirectML: moderado
        scheduler = "dpm++"  # DPM++ precisa menos steps
    elif device == "cuda":
        base_steps = 20  # NVIDIA: pode usar mais steps
        scheduler = "dpm++"
    else:  # CPU
        base_steps = 15  # CPU: bom equil√≠brio velocidade/qualidade
        scheduler = "dpm++"  # DPM++ √© mais eficiente
    
    configs = {
        'runwayml/stable-diffusion-v1-5': {
            'steps': base_steps,
            'guidance_scale': 7.5,
            'size': '512x512',
            'scheduler': scheduler,
            'memory_efficient': True
        },
        'stabilityai/sdxl-turbo': {
            'steps': 4 if device == "dml" else 6,
            'guidance_scale': 0.0,  # SDXL Turbo N√ÉO USA guidance
            'size': '512x512',
            'scheduler': 'euler_a',  # Euler A √© melhor para Turbo
            'memory_efficient': True,
            'is_turbo': True  # Flag para tratamento especial
        },
        'lykon/dreamshaper-8': {
            'steps': base_steps,
            'guidance_scale': 7.5,
            'size': '512x512',
            'scheduler': scheduler,
            'memory_efficient': True
        },
        'prompthero/openjourney': {
            'steps': base_steps,
            'guidance_scale': 7.5,
            'size': '512x512',
            'scheduler': scheduler,
            'memory_efficient': True
        },
        'Linaqruf/anything-v3.0': {
            'steps': base_steps,
            'guidance_scale': 7.5,
            'size': '512x512',
            'scheduler': scheduler,
            'memory_efficient': True
        }
    }
    return configs.get(model_name, configs['runwayml/stable-diffusion-v1-5'])

def get_pipe(model_name):
    """Carrega o pipeline do modelo com otimiza√ß√µes extremas"""
    
    # Mapear nomes curtos
    model_mapping = {
        'stable-diffusion-1.5': 'runwayml/stable-diffusion-v1-5',
        'sd-1.5': 'runwayml/stable-diffusion-v1-5',
        'sdxl-turbo': 'stabilityai/sdxl-turbo',
        'dreamshaper': 'lykon/dreamshaper-8',
        'openjourney': 'prompthero/openjourney',
        'anything-v3': 'Linaqruf/anything-v3.0',
        'FLUX.1-dev': 'runwayml/stable-diffusion-v1-5'
    }
    
    full_model_name = model_mapping.get(model_name, model_name)
    
    if full_model_name not in pipes:
        logger.info(f"Loading model: {full_model_name} (requested: {model_name})")
        try:
            device = detect_device()
            
            # Configurar dtype
            if device == "dml":
                torch_dtype = torch.float32
            elif device == "cuda":
                torch_dtype = torch.float16
            else:
                torch_dtype = torch.float32
            
            logger.info(f"Using device: {device}, dtype: {torch_dtype}")
            
            # Importar diffusers
            from diffusers import StableDiffusionPipeline
            
            # Carregar pipeline com safety_checker desabilitado (mais r√°pido)
            pipes[full_model_name] = StableDiffusionPipeline.from_pretrained(
                full_model_name, 
                torch_dtype=torch_dtype,
                safety_checker=None,  # Desabilitar safety checker (ganho de velocidade)
                requires_safety_checker=False,
                use_auth_token=os.getenv("HUGGINGFACE_HUB_TOKEN")
            )
            
            # Otimiza√ß√µes baseadas no device
            if device == "dml":
                try:
                    pipes[full_model_name] = pipes[full_model_name].to("dml")
                    logger.info("Using DirectML device: dml")
                except Exception as e:
                    logger.warning(f"Could not use DirectML with .to('dml'): {e}")
                    try:
                        import torch_directml
                        dml_device = torch_directml.device()
                        pipes[full_model_name] = pipes[full_model_name].to(dml_device)
                        logger.info(f"Using DirectML device: {dml_device}")
                    except Exception as e2:
                        logger.warning(f"Could not use DirectML, falling back to CPU: {e2}")
                        pipes[full_model_name] = pipes[full_model_name].to("cpu")
                        device = "cpu"
                
                if device == "dml":
                    # Otimiza√ß√µes CR√çTICAS para DirectML
                    pipes[full_model_name].enable_attention_slicing(1)
                    pipes[full_model_name].enable_vae_slicing()
                    logger.info("Applied AMD GPU optimizations (DirectML)")
                    
            elif device == "cuda":
                pipes[full_model_name] = pipes[full_model_name].to("cuda")
                pipes[full_model_name].enable_attention_slicing()
                pipes[full_model_name].enable_vae_slicing()
                # Tentar xformers se dispon√≠vel
                try:
                    pipes[full_model_name].enable_xformers_memory_efficient_attention()
                    logger.info("Applied NVIDIA GPU optimizations (with xformers)")
                except:
                    logger.info("Applied NVIDIA GPU optimizations (without xformers)")
                    
            else:  # CPU
                pipes[full_model_name] = pipes[full_model_name].to("cpu")
                pipes[full_model_name].enable_attention_slicing()
                pipes[full_model_name].enable_vae_slicing()
                
                # CPU offload apenas se accelerate estiver dispon√≠vel
                try:
                    pipes[full_model_name].enable_model_cpu_offload()
                    logger.info("Applied CPU optimizations with model offload")
                except:
                    logger.info("Applied CPU optimizations (standard mode)")
            
            # Aplicar scheduler otimizado
            model_config = get_model_config(full_model_name, device)
            pipes[full_model_name] = get_scheduler(
                pipes[full_model_name], 
                model_config['scheduler'],
                device
            )
            
            logger.info(f"‚úÖ Model {full_model_name} loaded successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Error loading model {full_model_name}: {e}")
            raise HTTPException(status_code=500, detail=f"Error loading model: {str(e)}")
    
    return pipes[full_model_name]

@app.get("/health")
def health_check():
    """Health check endpoint"""
    device = detect_device()
    return {
        "status": "healthy",
        "service": "APIBR2 Ultra Optimized Image Generation v2.0",
        "device": device,
        "cpu": "AMD Ryzen 9 7900X (12-Core)",
        "gpu": "AMD Radeon RX 6750 XT (12GB)" if device == "dml" else "N/A",
        "ram": "32GB DDR5 5600MHz",
        "timestamp": datetime.now().isoformat(),
        "loaded_models": list(pipes.keys()),
        "optimizations": {
            "scheduler": "DPM++ 2M (default)",
            "attention_slicing": "enabled",
            "vae_slicing": "enabled",
            "safety_checker": "disabled"
        }
    }

@app.post("/generate")
def generate_image(req: ImageRequest):
    """Endpoint para gera√ß√£o de imagem com otimiza√ß√µes ultra v2.0"""
    try:
        logger.info(f"üé® Generating: {req.prompt[:50]}... | Model: {req.model}")
        
        start_time = time.time()
        current_device = detect_device()
        
        # Obter configura√ß√µes otimizadas
        model_config = get_model_config(req.model, current_device)
        
        # Usar steps otimizados se for padr√£o
        if req.steps == 10:
            req.steps = model_config['steps']
        
        # Limitar steps para DirectML apenas se for muito alto
        if current_device == "dml" and req.steps > 25:
            logger.warning(f"DirectML: Limiting steps from {req.steps} to 25 for performance")
            req.steps = 25
        
        # Carregar pipeline
        pipe = get_pipe(req.model)
        
        # Atualizar scheduler se especificado
        if req.scheduler != "auto":
            pipe = get_scheduler(pipe, req.scheduler, current_device)
        
        # Limpar mem√≥ria
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Validar tamanho
        if current_device == "dml":
            max_size = 512
        elif current_device == "cuda":
            max_size = 768
        else:
            max_size = 640  # CPU pode fazer um pouco mais
        
        original_width = req.width
        original_height = req.height
        
        if req.width > max_size or req.height > max_size:
            logger.warning(f"Size {req.width}x{req.height} exceeds max {max_size}x{max_size}")
            if req.width > req.height:
                req.width = max_size
                req.height = int((req.height / original_width) * max_size)
            else:
                req.height = max_size
                req.width = int((req.width / original_height) * max_size)
        
        # M√∫ltiplos de 8
        req.width = (req.width // 8) * 8
        req.height = (req.height // 8) * 8
        
        # Tamanho m√≠nimo
        req.width = max(req.width, 256)
        req.height = max(req.height, 256)
        
        logger.info(f"üìê Size: {req.width}x{req.height} | Steps: {req.steps} | Device: {current_device}")
        
        # Estimar tempo
        if current_device == "dml":
            estimated_time = req.steps * 3  # ~3s por step no DirectML
            logger.warning(f"‚è±Ô∏è DirectML: Estimated time ~{estimated_time}s")
        elif current_device == "cpu":
            estimated_time = req.steps * 2  # ~2s por step no Ryzen 9 7900X
            logger.info(f"‚è±Ô∏è CPU: Estimated time ~{estimated_time}s")
        else:
            estimated_time = req.steps * 0.5  # ~0.5s por step no CUDA
            logger.info(f"‚è±Ô∏è GPU: Estimated time ~{estimated_time}s")
        
        # Gerar imagem
        try:
            # Configura√ß√£o de guidance especial para SDXL Turbo
            guidance = req.guidance_scale
            if "turbo" in req.model.lower():
                guidance = 0.0
            
            result = pipe(
                req.prompt,
                num_inference_steps=req.steps,
                guidance_scale=guidance,
                height=req.height,
                width=req.width
            )
            
        except Exception as gen_error:
            error_str = str(gen_error).lower()
            
            # Tratamento de erros de mem√≥ria
            if "memory" in error_str or "not enough" in error_str or "allocate" in error_str:
                logger.warning(f"‚ö†Ô∏è Memory error: {gen_error}")
                
                # Tentar tamanho reduzido
                if req.width > 512 or req.height > 512:
                    logger.info("Trying reduced size 512x512...")
                    req.width = 512
                    req.height = 512
                    try:
                        result = pipe(
                            req.prompt,
                            num_inference_steps=req.steps,
                            guidance_scale=guidance,
                            height=req.height,
                            width=req.width
                        )
                        logger.info("‚úÖ Generation completed with reduced size")
                    except:
                        # Fallback para CPU
                        logger.info("Falling back to CPU...")
                        pipe_cpu = pipe.to("cpu")
                        result = pipe_cpu(
                            req.prompt,
                            num_inference_steps=req.steps,
                            guidance_scale=guidance,
                            height=req.height,
                            width=req.width
                        )
                        logger.info("‚úÖ Generation completed on CPU")
                else:
                    # J√° em tamanho m√≠nimo, ir para CPU
                    logger.info("Falling back to CPU...")
                    pipe_cpu = pipe.to("cpu")
                    result = pipe_cpu(
                        req.prompt,
                        num_inference_steps=req.steps,
                        guidance_scale=guidance,
                        height=req.height,
                        width=req.width
                    )
                    logger.info("‚úÖ Generation completed on CPU")
                    
            elif "dml" in error_str or "privateuseone" in error_str:
                logger.warning(f"‚ö†Ô∏è DirectML error: {gen_error}")
                logger.info("Falling back to CPU...")
                pipe_cpu = pipe.to("cpu")
                result = pipe_cpu(
                    req.prompt,
                    num_inference_steps=req.steps,
                    guidance_scale=guidance,
                    height=req.height,
                    width=req.width
                )
                logger.info("‚úÖ Generation completed on CPU (DirectML fallback)")
            else:
                raise
        
        image = result.images[0]
        generation_time = time.time() - start_time
        
        # Salvar imagem
        timestamp = int(time.time())
        model_short_name = req.model.split('/')[-1] if '/' in req.model else req.model
        filename = f"{model_short_name}_{timestamp}_{uuid.uuid4().hex[:8]}.png"
        filepath = OUT_DIR / filename
        
        image.save(filepath)
        logger.info(f"‚úÖ Image saved: {filename} | Time: {generation_time:.2f}s")
        
        # Converter para base64
        with open(filepath, "rb") as image_file:
            image_base64 = base64.b64encode(image_file.read()).decode('utf-8')
        
        # Limpar mem√≥ria
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return {
            "success": True,
            "data": {
                "image_base64": image_base64,
                "image_url": f"http://apibr.giesel.com.br/images/{filename}",
                "local_path": str(filepath),
                "prompt": req.prompt,
                "model": req.model,
                "size": f"{req.width}x{req.height}",
                "timestamp": datetime.now().isoformat()
            },
            "metadata": {
                "model": req.model,
                "generation_time": round(generation_time, 2),
                "steps": req.steps,
                "guidance_scale": req.guidance_scale,
                "scheduler": req.scheduler,
                "device": current_device,
                "optimization_level": "ultra_v2",
                "estimated_vs_actual": f"{estimated_time}s vs {generation_time:.1f}s",
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error: {str(e)}")
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
def list_models():
    """Listar modelos com configura√ß√µes otimizadas v2.0"""
    device = detect_device()
    
    # Performance estimada baseada no device
    if device == "dml":
        perf_512 = "25-40s"
        perf_note = "DirectML (lento mas funcional)"
    elif device == "cpu":
        perf_512 = "20-30s"
        perf_note = "CPU Ryzen 9 7900X (r√°pido!)"
    else:
        perf_512 = "5-10s"
        perf_note = "GPU CUDA (muito r√°pido)"
    
    return {
        "models": {
            "runwayml/stable-diffusion-v1-5": {
                "name": "Stable Diffusion 1.5",
                "alias": ["sd-1.5", "stable-diffusion-1.5"],
                "description": "Modelo base, vers√°til e r√°pido",
                "recommended_steps": 15,
                "recommended_scheduler": "dpm++",
                "recommended_size": "512x512",
                "performance_512x512": perf_512
            },
            "stabilityai/sdxl-turbo": {
                "name": "SDXL Turbo",
                "alias": ["sdxl-turbo"],
                "description": "Ultra-r√°pido, 4-6 steps",
                "recommended_steps": 10,
                "recommended_scheduler": "euler_a",
                "recommended_size": "512x512",
                "performance_512x512": "10-15s"
            },
            "lykon/dreamshaper-8": {
                "name": "DreamShaper 8",
                "alias": ["dreamshaper"],
                "description": "Estilo art√≠stico/realista",
                "recommended_steps": 15,
                "recommended_scheduler": "dpm++",
                "recommended_size": "512x512",
                "performance_512x512": perf_512
            },
            "prompthero/openjourney": {
                "name": "OpenJourney",
                "alias": ["openjourney"],
                "description": "Estilo Midjourney",
                "recommended_steps": 15,
                "recommended_scheduler": "dpm++",
                "recommended_size": "512x512",
                "performance_512x512": perf_512
            },
            "Linaqruf/anything-v3.0": {
                "name": "Anything V3",
                "alias": ["anything-v3"],
                "description": "Anime/Manga style",
                "recommended_steps": 15,
                "recommended_scheduler": "dpm++",
                "recommended_size": "512x512",
                "performance_512x512": perf_512
            }
        },
        "schedulers": {
            "dpm++": "DPM++ 2M (recomendado - r√°pido e qualidade)",
            "euler_a": "Euler Ancestral (bom para SDXL Turbo)",
            "ddim": "DDIM (cl√°ssico, mais lento)",
            "pndm": "PNDM (padr√£o original)"
        },
        "device_info": {
            "current_device": device,
            "device_note": perf_note,
            "cpu": "AMD Ryzen 9 7900X (12-Core, 24-Threads)",
            "cpu_threads_configured": num_threads,
            "gpu": "AMD Radeon RX 6750 XT (12GB GDDR6)" if device == "dml" else "N/A",
            "optimization_level": "ultra_v2.1",
            "optimizations_enabled": [
                "DPM++ Scheduler",
                "Attention Slicing",
                "VAE Slicing",
                "Safety Checker Disabled",
                "Memory Management",
                f"Multi-threading ({num_threads} threads)"
            ]
        },
        "tips": {
            "fastest": "Use SDXL Turbo com 4-6 steps (guidance_scale=0.0)",
            "best_quality": "Use SD 1.5 com 20-30 steps",
            "balanced": "Use SD 1.5 com 15 steps (recomendado)",
            "development": "Set PREFER_CPU=true para usar Ryzen (mais est√°vel)",
            "directml_slow": "DirectML √© lento, considere CPU para testes r√°pidos",
            "negative_prompt": "Use negative_prompt para melhorar qualidade"
        }
    }

@app.get("/images/{filename}")
def serve_image(filename: str):
    """Servir imagens geradas"""
    try:
        filepath = OUT_DIR / filename
        if filepath.exists():
            return FileResponse(filepath, media_type='image/png')
        else:
            raise HTTPException(status_code=404, detail="Image not found")
    except Exception as e:
        logger.error(f"Error serving image: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/benchmark")
def benchmark_info():
    """Informa√ß√µes de benchmark do sistema"""
    device = detect_device()
    
    benchmarks = {
        "system": {
            "cpu": "AMD Ryzen 9 7900X",
            "cores": "12-Core / 24-Threads",
            "threads_configured": num_threads,
            "base_clock": "4.7 GHz",
            "boost_clock": "5.6 GHz",
            "gpu": "AMD Radeon RX 6750 XT",
            "vram": "12GB GDDR6",
            "ram": "32GB DDR5 5600MHz"
        },
        "estimated_performance": {
            "cpu_512x512_15steps": "25-35 segundos (otimizado multi-thread)",
            "cpu_512x512_20steps": "35-50 segundos (otimizado multi-thread)",
            "cpu_512x512_30steps": "50-75 segundos (otimizado multi-thread)",
            "dml_512x512_12steps": "40-60 segundos (inst√°vel)",
            "dml_512x512_20steps": "70-100 segundos (inst√°vel)",
            "note": "CPU com multi-threading otimizado - esperado 80-100% de uso agora"
        },
        "recommendations": {
            "development": "Use CPU (set PREFER_CPU=true)",
            "production": "Use CPU para estabilidade ou CUDA se tiver NVIDIA",
            "fastest_model": "SDXL Turbo com 6 steps (~10-15s)",
            "best_quality": "SD 1.5 com 15-20 steps (~30-60s)"
        },
        "current_device": device
    }
    
    return benchmarks

if __name__ == "__main__":
    import uvicorn
    logger.info("=" * 60)
    logger.info("üöÄ Starting APIBR2 Ultra Optimized v2.1")
    logger.info("=" * 60)
    logger.info(f"üìÅ Output directory: {OUT_DIR.absolute()}")
    logger.info(f"üíª CPU: AMD Ryzen 9 7900X (12-Core, {num_threads} threads)")
    logger.info(f"üéÆ GPU: AMD Radeon RX 6750 XT (12GB)")
    logger.info(f"üîß Device: {detect_device()}")
    logger.info(f"‚ú® Optimizations: DPM++, Multi-threading, Attention Slicing, VAE Slicing")
    logger.info("=" * 60)
    logger.info("üí° Tips:")
    logger.info("   - Para usar CPU (recomendado): set PREFER_CPU=true")
    logger.info("   - Para for√ßar CPU: set FORCE_CPU=true")
    logger.info("   - DirectML funciona mas √© MUITO lento")
    logger.info("=" * 60)
    uvicorn.run(app, host="0.0.0.0", port=5001)